{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weighted-attention",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning Implementation Examples\n",
    "Note that these examples are mostly to serve as an example of how a properly executed training loop should look. It does not include evaluation (which would simply be a call to the network), and it generally assumes the average parameter for most hyperparamters. \n",
    "\n",
    "A proper training protocol should include a hyperparamter sweep for whichever parameters requirement, and it should optimally include the __@tf.function__ decorator to speed up the training/inference loops whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personal-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only needs to be run once (it takes some time to initialize the first time you use it)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Only useful for GPU devices\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "independent-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the required imports for this example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# HYPERPARAMETERS FOR DATASET\n",
    "PERCENT_LABELED = 0.10\n",
    "\n",
    "# HYPERPARAMETERS FOR NETWORK\n",
    "DISTANCE_WEIGHT_MAX = 30\n",
    "DISTANCE_MAX_WEIGHT_EPOCH = 10\n",
    "TRAINING_EPOCHS = 50\n",
    "BATCH_SIZE = 1024 # unlabeled batch_size determined proportionally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-chrome",
   "metadata": {},
   "source": [
    "# Initialization of Model Data\n",
    "##### Keep in mind that that one of the central points of SSL is to train on an unsupervised dataset that is constantly being augmented. While the augmentations aren't carried out here, the Dataset class in TensorFlow does allow for online augmentations to take place. (See [this](https://www.tensorflow.org/tutorials/images/data_augmentation) link.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considerable-messenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of labeled dataset: 6000\n",
      "Size of unlabeled dataset: 54000\n"
     ]
    }
   ],
   "source": [
    "# Loads in the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Generate 32x32 image set for use in LeNet\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2))).reshape((-1, 32, 32, 1))\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2))).reshape((-1, 32, 32, 1))\n",
    "\n",
    "# Normalize input values\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "\n",
    "# Separate dataset into labeled and unlabeled portions\n",
    "sample_inds = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(sample_inds)\n",
    "split_ind = int(PERCENT_LABELED*x_train.shape[0])\n",
    "x_train_labeled = x_train[sample_inds[:split_ind],:,:]\n",
    "y_train_labeled = y_train[sample_inds[:split_ind]].astype('float32')\n",
    "x_train_unlabeled = x_train[sample_inds[split_ind:],:,:]\n",
    "\n",
    "# Generate datasets with batching from samples\n",
    "l_train_ds = tf.data.Dataset.from_tensor_slices((x_train_labeled, y_train_labeled))\n",
    "ul_train_ds = tf.data.Dataset.from_tensor_slices(x_train_unlabeled)\n",
    "\n",
    "l_train_ds = l_train_ds.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "num_batches = int((x_train_labeled.shape[0]+BATCH_SIZE-1)/BATCH_SIZE)\n",
    "ul_train_ds = ul_train_ds.shuffle(buffer_size=1024).batch(int(x_train_unlabeled.shape[0]/num_batches), drop_remainder=True)\n",
    "\n",
    "print(\"Size of labeled dataset: {}\".format(x_train_labeled.shape[0]))\n",
    "print(\"Size of unlabeled dataset: {}\".format(x_train_unlabeled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inside-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model to use (the actual model doesn't matter so long as it has dropout)\n",
    "def LeNet(dropout_prob = 0.5):\n",
    "    # input\n",
    "    xIn = keras.Input(shape=(32,32,1))\n",
    "    \n",
    "    # subsequent layers\n",
    "    out = keras.layers.Conv2D(6, 5, activation='relu')(xIn)\n",
    "    out = keras.layers.AveragePooling2D(2, 2)(out)\n",
    "    out = keras.layers.Conv2D(16, 5, activation='relu')(out)\n",
    "    out = keras.layers.AveragePooling2D(2, 2)(out)\n",
    "    out = keras.layers.Flatten()(out)\n",
    "    out = keras.layers.Dense(120, activation='relu')(out)\n",
    "    out = keras.layers.Dropout(dropout_prob)(out)\n",
    "    out = keras.layers.Dense(84, activation='relu')(out)\n",
    "    out = keras.layers.Dropout(dropout_prob)(out)\n",
    "    out = keras.layers.Dense(10, name='prelogits')(out)\n",
    "    out = keras.layers.Activation('softmax', name='logits')(out)\n",
    "    \n",
    "    # Creates model\n",
    "    mod = keras.Model(inputs=xIn, outputs=out)\n",
    "    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-hostel",
   "metadata": {},
   "source": [
    "### PI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tribal-literature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.1731, US_Loss: 0.0170, S_Loss: 2.1561, Acc: 0.24\n",
      "Epoch 2\n",
      "Loss: 1.7144, US_Loss: 0.1984, S_Loss: 1.5160, Acc: 0.56\n",
      "Epoch 3\n",
      "Loss: 1.3375, US_Loss: 0.3267, S_Loss: 1.0109, Acc: 0.72\n",
      "Epoch 4\n",
      "Loss: 1.0206, US_Loss: 0.3159, S_Loss: 0.7047, Acc: 0.80\n",
      "Epoch 5\n",
      "Loss: 0.8117, US_Loss: 0.2695, S_Loss: 0.5421, Acc: 0.85\n",
      "Epoch 6\n",
      "Loss: 0.6637, US_Loss: 0.2410, S_Loss: 0.4227, Acc: 0.89\n",
      "Epoch 7\n",
      "Loss: 0.5466, US_Loss: 0.2011, S_Loss: 0.3456, Acc: 0.91\n",
      "Epoch 8\n",
      "Loss: 0.4819, US_Loss: 0.1881, S_Loss: 0.2938, Acc: 0.93\n",
      "Epoch 9\n",
      "Loss: 0.4156, US_Loss: 0.1670, S_Loss: 0.2486, Acc: 0.94\n",
      "Epoch 10\n",
      "Loss: 0.3728, US_Loss: 0.1504, S_Loss: 0.2224, Acc: 0.94\n",
      "Epoch 11\n",
      "Loss: 0.3400, US_Loss: 0.1433, S_Loss: 0.1967, Acc: 0.95\n",
      "Epoch 12\n",
      "Loss: 0.3027, US_Loss: 0.1270, S_Loss: 0.1756, Acc: 0.95\n",
      "Epoch 13\n",
      "Loss: 0.2729, US_Loss: 0.1174, S_Loss: 0.1554, Acc: 0.96\n",
      "Epoch 14\n",
      "Loss: 0.2574, US_Loss: 0.1126, S_Loss: 0.1448, Acc: 0.96\n",
      "Epoch 15\n",
      "Loss: 0.2238, US_Loss: 0.1022, S_Loss: 0.1217, Acc: 0.97\n",
      "Epoch 16\n",
      "Loss: 0.2172, US_Loss: 0.0987, S_Loss: 0.1185, Acc: 0.97\n",
      "Epoch 17\n",
      "Loss: 0.2082, US_Loss: 0.0953, S_Loss: 0.1129, Acc: 0.97\n",
      "Epoch 18\n",
      "Loss: 0.1940, US_Loss: 0.0926, S_Loss: 0.1014, Acc: 0.97\n",
      "Epoch 19\n",
      "Loss: 0.1764, US_Loss: 0.0882, S_Loss: 0.0882, Acc: 0.98\n",
      "Epoch 20\n",
      "Loss: 0.1718, US_Loss: 0.0856, S_Loss: 0.0862, Acc: 0.98\n",
      "Epoch 21\n",
      "Loss: 0.1595, US_Loss: 0.0787, S_Loss: 0.0808, Acc: 0.98\n",
      "Epoch 22\n",
      "Loss: 0.1472, US_Loss: 0.0737, S_Loss: 0.0735, Acc: 0.98\n",
      "Epoch 23\n",
      "Loss: 0.1457, US_Loss: 0.0738, S_Loss: 0.0718, Acc: 0.98\n",
      "Epoch 24\n",
      "Loss: 0.1374, US_Loss: 0.0725, S_Loss: 0.0649, Acc: 0.98\n",
      "Epoch 25\n",
      "Loss: 0.1389, US_Loss: 0.0740, S_Loss: 0.0649, Acc: 0.98\n",
      "Epoch 26\n",
      "Loss: 0.1214, US_Loss: 0.0675, S_Loss: 0.0539, Acc: 0.98\n",
      "Epoch 27\n",
      "Loss: 0.1279, US_Loss: 0.0680, S_Loss: 0.0599, Acc: 0.98\n",
      "Epoch 28\n",
      "Loss: 0.1253, US_Loss: 0.0681, S_Loss: 0.0572, Acc: 0.99\n",
      "Epoch 29\n",
      "Loss: 0.1178, US_Loss: 0.0661, S_Loss: 0.0517, Acc: 0.99\n",
      "Epoch 30\n",
      "Loss: 0.1174, US_Loss: 0.0647, S_Loss: 0.0528, Acc: 0.99\n",
      "Epoch 31\n",
      "Loss: 0.1120, US_Loss: 0.0636, S_Loss: 0.0483, Acc: 0.99\n",
      "Epoch 32\n",
      "Loss: 0.1074, US_Loss: 0.0607, S_Loss: 0.0467, Acc: 0.99\n",
      "Epoch 33\n",
      "Loss: 0.1060, US_Loss: 0.0589, S_Loss: 0.0471, Acc: 0.99\n",
      "Epoch 34\n",
      "Loss: 0.1043, US_Loss: 0.0612, S_Loss: 0.0431, Acc: 0.99\n",
      "Epoch 35\n",
      "Loss: 0.0972, US_Loss: 0.0581, S_Loss: 0.0390, Acc: 0.99\n",
      "Epoch 36\n",
      "Loss: 0.0895, US_Loss: 0.0558, S_Loss: 0.0337, Acc: 0.99\n",
      "Epoch 37\n",
      "Loss: 0.0866, US_Loss: 0.0536, S_Loss: 0.0330, Acc: 0.99\n",
      "Epoch 38\n",
      "Loss: 0.0897, US_Loss: 0.0538, S_Loss: 0.0359, Acc: 0.99\n",
      "Epoch 39\n",
      "Loss: 0.0837, US_Loss: 0.0538, S_Loss: 0.0298, Acc: 0.99\n",
      "Epoch 40\n",
      "Loss: 0.0847, US_Loss: 0.0543, S_Loss: 0.0303, Acc: 0.99\n",
      "Epoch 41\n",
      "Loss: 0.0809, US_Loss: 0.0522, S_Loss: 0.0288, Acc: 0.99\n",
      "Epoch 42\n",
      "Loss: 0.0825, US_Loss: 0.0504, S_Loss: 0.0321, Acc: 0.99\n",
      "Epoch 43\n",
      "Loss: 0.0797, US_Loss: 0.0507, S_Loss: 0.0289, Acc: 0.99\n",
      "Epoch 44\n",
      "Loss: 0.0796, US_Loss: 0.0486, S_Loss: 0.0310, Acc: 0.99\n",
      "Epoch 45\n",
      "Loss: 0.0786, US_Loss: 0.0495, S_Loss: 0.0291, Acc: 0.99\n",
      "Epoch 46\n",
      "Loss: 0.0734, US_Loss: 0.0485, S_Loss: 0.0249, Acc: 0.99\n",
      "Epoch 47\n",
      "Loss: 0.0737, US_Loss: 0.0467, S_Loss: 0.0270, Acc: 0.99\n",
      "Epoch 48\n",
      "Loss: 0.0723, US_Loss: 0.0470, S_Loss: 0.0253, Acc: 0.99\n",
      "Epoch 49\n",
      "Loss: 0.0685, US_Loss: 0.0490, S_Loss: 0.0195, Acc: 1.00\n",
      "Epoch 50\n",
      "Loss: 0.0665, US_Loss: 0.0457, S_Loss: 0.0209, Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Implement PI-Network Training\n",
    "curModel = LeNet()\n",
    "sup_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-3)\n",
    "epochVar = tf.Variable(0)\n",
    "\n",
    "# Metrics\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "unsup_loss_metric = keras.metrics.Mean()\n",
    "sup_loss_metric = keras.metrics.Mean()\n",
    "total_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "# Implement the values chosen for the weighting function\n",
    "w_values = tf.constant(np.linspace(0, DISTANCE_WEIGHT_MAX, DISTANCE_MAX_WEIGHT_EPOCH), dtype='float32')\n",
    "\n",
    "# Now set up weighing function\n",
    "@tf.function\n",
    "def weigh_fn(epoch):\n",
    "    return w_values[tf.math.minimum(DISTANCE_MAX_WEIGHT_EPOCH-1, epoch)]\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_sup, x_unsup, y_sup, epoch):\n",
    "    # Calculate gradients while performing operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Process the necessary values for each batch\n",
    "        l_logits = curModel(x_sup, training=True)\n",
    "        us_logits_1 = curModel(x_unsup, training=True)\n",
    "        us_logits_2 = curModel(x_unsup, training=True)\n",
    "\n",
    "        # Compute losses for each respective dataset\n",
    "        sup_loss = sup_loss_fn(y_sup, l_logits)\n",
    "        unsup_loss = weigh_fn(epoch) * tf.reduce_mean(keras.losses.MSE(us_logits_2, us_logits_1))\n",
    "        total_loss = sup_loss + unsup_loss\n",
    "        \n",
    "        # Update relevant metrics\n",
    "        train_acc_metric.update_state(y_sup, l_logits)\n",
    "        unsup_loss_metric.update_state(unsup_loss)\n",
    "        sup_loss_metric.update_state(sup_loss)\n",
    "        total_loss_metric.update_state(total_loss)\n",
    "        \n",
    "    # Use tape to propagate gradients back to weights\n",
    "    grads = tape.gradient(total_loss, curModel.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, curModel.trainable_weights))\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    \n",
    "    # iterate accross the batches\n",
    "    for step, (labeled_batch, unlabeled_batch) in enumerate(zip(l_train_ds, ul_train_ds)):\n",
    "        labeled_bx, labeled_by = labeled_batch\n",
    "        unsup_samples = tf.concat([labeled_bx, unlabeled_batch], 0)\n",
    "        \n",
    "        # Progress through a step of training\n",
    "        train_step(labeled_bx, unsup_samples, labeled_by, epochVar)\n",
    "        \n",
    "        # Reset metrics and evaluate results\n",
    "        t_acc = train_acc_metric.result()\n",
    "        us_loss = unsup_loss_metric.result()\n",
    "        s_loss = sup_loss_metric.result()\n",
    "        t_loss = total_loss_metric.result()\n",
    "        \n",
    "        # Print epoch final statistics\n",
    "        if(step == num_batches-1):\n",
    "            print(\"Loss: {:.4f}, US_Loss: {:.4f}, S_Loss: {:.4f}, Acc: {:.2f}\".format(t_loss, us_loss,\n",
    "                                                                                      s_loss, t_acc))\n",
    "        epochVar = epochVar + 1\n",
    "    \n",
    "    # Reset states\n",
    "    train_acc_metric.reset_states()\n",
    "    unsup_loss_metric.reset_states()\n",
    "    total_loss_metric.reset_states()\n",
    "    sup_loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-liberty",
   "metadata": {},
   "source": [
    "# How Do We Implement Alternatives?\n",
    "##### Note that no observations were made about temporal ensembling with minibatches. It's quite possible that the stochasticity introduced by minibatches may cause some future instability issues with certain problems...\n",
    "\n",
    "The alternatives are basically the same! There are some small differences depending on what you implement though... For example, the following is the implementation of the temporal ensembling (it's not efficient, so don't train large networks like this!!!! A proper implementation would require managing resources a bit more than what is done here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-triple",
   "metadata": {},
   "source": [
    "### Temporal Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifteen-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling is disabled to allow for proper recording of outputs... Otherwise we would be required\n",
    "# to record shuffled indices on every iteration (which means implementing our own shuffles!)\n",
    "l_train_ds = tf.data.Dataset.from_tensor_slices((x_train_labeled, y_train_labeled))\n",
    "ul_train_ds = tf.data.Dataset.from_tensor_slices(x_train_unlabeled)\n",
    "\n",
    "l_train_ds = l_train_ds.batch(BATCH_SIZE)\n",
    "num_batches = int((x_train_labeled.shape[0]+BATCH_SIZE-1)/BATCH_SIZE)\n",
    "ul_train_ds = ul_train_ds.batch(int(x_train_unlabeled.shape[0]/num_batches), drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twelve-straight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.0106, US_Loss: 0.0000, S_Loss: 2.0106, Acc: 0.32\n",
      "Epoch 2\n",
      "Loss: 1.2828, US_Loss: 0.0751, S_Loss: 1.2077, Acc: 0.60\n",
      "Epoch 3\n",
      "Loss: 1.1087, US_Loss: 0.1922, S_Loss: 0.9165, Acc: 0.71\n",
      "Epoch 4\n",
      "Loss: 1.0641, US_Loss: 0.3493, S_Loss: 0.7148, Acc: 0.80\n",
      "Epoch 5\n",
      "Loss: 1.1460, US_Loss: 0.3819, S_Loss: 0.7640, Acc: 0.85\n",
      "Epoch 6\n",
      "Loss: 1.2186, US_Loss: 0.4024, S_Loss: 0.8162, Acc: 0.86\n",
      "Epoch 7\n",
      "Loss: 1.2898, US_Loss: 0.3629, S_Loss: 0.9269, Acc: 0.88\n",
      "Epoch 8\n",
      "Loss: 1.3041, US_Loss: 0.3525, S_Loss: 0.9515, Acc: 0.90\n",
      "Epoch 9\n",
      "Loss: 1.3541, US_Loss: 0.3346, S_Loss: 1.0194, Acc: 0.92\n",
      "Epoch 10\n",
      "Loss: 1.4116, US_Loss: 0.3357, S_Loss: 1.0759, Acc: 0.92\n",
      "Epoch 11\n",
      "Loss: 1.3707, US_Loss: 0.3482, S_Loss: 1.0225, Acc: 0.92\n",
      "Epoch 12\n",
      "Loss: 1.3572, US_Loss: 0.3476, S_Loss: 1.0097, Acc: 0.95\n",
      "Epoch 13\n",
      "Loss: 1.3503, US_Loss: 0.3477, S_Loss: 1.0026, Acc: 0.95\n",
      "Epoch 14\n",
      "Loss: 1.3434, US_Loss: 0.3449, S_Loss: 0.9985, Acc: 0.95\n",
      "Epoch 15\n",
      "Loss: 1.3206, US_Loss: 0.3463, S_Loss: 0.9744, Acc: 0.96\n",
      "Epoch 16\n",
      "Loss: 1.3240, US_Loss: 0.3494, S_Loss: 0.9746, Acc: 0.95\n",
      "Epoch 17\n",
      "Loss: 1.3021, US_Loss: 0.3617, S_Loss: 0.9404, Acc: 0.96\n",
      "Epoch 18\n",
      "Loss: 1.3084, US_Loss: 0.3511, S_Loss: 0.9572, Acc: 0.96\n",
      "Epoch 19\n",
      "Loss: 1.2872, US_Loss: 0.3539, S_Loss: 0.9333, Acc: 0.97\n",
      "Epoch 20\n",
      "Loss: 1.2884, US_Loss: 0.3548, S_Loss: 0.9337, Acc: 0.97\n",
      "Epoch 21\n",
      "Loss: 1.2684, US_Loss: 0.3589, S_Loss: 0.9094, Acc: 0.98\n",
      "Epoch 22\n",
      "Loss: 1.2961, US_Loss: 0.3512, S_Loss: 0.9449, Acc: 0.97\n",
      "Epoch 23\n",
      "Loss: 1.2562, US_Loss: 0.3654, S_Loss: 0.8907, Acc: 0.98\n",
      "Epoch 24\n",
      "Loss: 1.2759, US_Loss: 0.3571, S_Loss: 0.9188, Acc: 0.98\n",
      "Epoch 25\n",
      "Loss: 1.2589, US_Loss: 0.3688, S_Loss: 0.8900, Acc: 0.97\n",
      "Epoch 26\n",
      "Loss: 1.2716, US_Loss: 0.3587, S_Loss: 0.9129, Acc: 0.98\n",
      "Epoch 27\n",
      "Loss: 1.2531, US_Loss: 0.3636, S_Loss: 0.8895, Acc: 0.97\n",
      "Epoch 28\n",
      "Loss: 1.2621, US_Loss: 0.3730, S_Loss: 0.8890, Acc: 0.98\n",
      "Epoch 29\n",
      "Loss: 1.2482, US_Loss: 0.3637, S_Loss: 0.8845, Acc: 0.99\n",
      "Epoch 30\n",
      "Loss: 1.2563, US_Loss: 0.3687, S_Loss: 0.8876, Acc: 0.97\n",
      "Epoch 31\n",
      "Loss: 1.2436, US_Loss: 0.3733, S_Loss: 0.8702, Acc: 0.99\n",
      "Epoch 32\n",
      "Loss: 1.2354, US_Loss: 0.3652, S_Loss: 0.8702, Acc: 0.98\n",
      "Epoch 33\n",
      "Loss: 1.2479, US_Loss: 0.3742, S_Loss: 0.8737, Acc: 0.98\n",
      "Epoch 34\n",
      "Loss: 1.2388, US_Loss: 0.3712, S_Loss: 0.8677, Acc: 0.99\n",
      "Epoch 35\n",
      "Loss: 1.2251, US_Loss: 0.3771, S_Loss: 0.8480, Acc: 0.99\n",
      "Epoch 36\n",
      "Loss: 1.2318, US_Loss: 0.3637, S_Loss: 0.8681, Acc: 0.99\n",
      "Epoch 37\n",
      "Loss: 1.2329, US_Loss: 0.3794, S_Loss: 0.8536, Acc: 0.99\n",
      "Epoch 38\n",
      "Loss: 1.2331, US_Loss: 0.3769, S_Loss: 0.8563, Acc: 0.98\n",
      "Epoch 39\n",
      "Loss: 1.2294, US_Loss: 0.3596, S_Loss: 0.8698, Acc: 0.99\n",
      "Epoch 40\n",
      "Loss: 1.2332, US_Loss: 0.3763, S_Loss: 0.8569, Acc: 0.99\n",
      "Epoch 41\n",
      "Loss: 1.2324, US_Loss: 0.3745, S_Loss: 0.8579, Acc: 0.98\n",
      "Epoch 42\n",
      "Loss: 1.2134, US_Loss: 0.3794, S_Loss: 0.8340, Acc: 0.99\n",
      "Epoch 43\n",
      "Loss: 1.2328, US_Loss: 0.3664, S_Loss: 0.8664, Acc: 0.99\n",
      "Epoch 44\n",
      "Loss: 1.2132, US_Loss: 0.3812, S_Loss: 0.8320, Acc: 0.99\n",
      "Epoch 45\n",
      "Loss: 1.2337, US_Loss: 0.3650, S_Loss: 0.8687, Acc: 0.98\n",
      "Epoch 46\n",
      "Loss: 1.2254, US_Loss: 0.3739, S_Loss: 0.8515, Acc: 0.99\n",
      "Epoch 47\n",
      "Loss: 1.2259, US_Loss: 0.3778, S_Loss: 0.8482, Acc: 0.98\n",
      "Epoch 48\n",
      "Loss: 1.2043, US_Loss: 0.3646, S_Loss: 0.8396, Acc: 0.99\n",
      "Epoch 49\n",
      "Loss: 1.2101, US_Loss: 0.3844, S_Loss: 0.8257, Acc: 0.99\n",
      "Epoch 50\n",
      "Loss: 1.2190, US_Loss: 0.3706, S_Loss: 0.8484, Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Implement Temporal Ensembling...\n",
    "curModel = LeNet()\n",
    "sup_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-3)\n",
    "epochVar = tf.Variable(0)\n",
    "stepVar = tf.Variable(0)\n",
    "\n",
    "# Metrics\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "unsup_loss_metric = keras.metrics.Mean()\n",
    "sup_loss_metric = keras.metrics.Mean()\n",
    "total_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "# Implement the values chosen for the weighting function\n",
    "w_values = tf.constant(np.linspace(0, DISTANCE_WEIGHT_MAX, DISTANCE_MAX_WEIGHT_EPOCH), dtype='float32')\n",
    "\n",
    "# Now set up weighing function\n",
    "@tf.function\n",
    "def weigh_fn(epoch):\n",
    "    return w_values[tf.math.minimum(DISTANCE_MAX_WEIGHT_EPOCH-1, epoch)]\n",
    "\n",
    "# Now set up EMA variables\n",
    "EMA_RATE = tf.constant(0.5)\n",
    "yEMA = tf.zeros((x_train.shape[0], 10))\n",
    "totalSamples = tf.constant(x_train.shape[0])\n",
    "totalBatch = tf.constant(int(x_train_unlabeled.shape[0]/num_batches) + BATCH_SIZE)\n",
    "epLogits = tf.Variable(0., shape=tf.TensorShape(None))\n",
    "\n",
    "# Now set up the EMA function\n",
    "@tf.function\n",
    "def EMA_fn(toUpdate, update, epoch):\n",
    "    return (EMA_RATE*toUpdate + (1-EMA_RATE)*update)/(1-tf.math.pow(EMA_RATE,tf.cast(epoch, dtype='float32')))\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_sup, x_unsup, y_sup, epoch, step):\n",
    "    # Calculate gradients while performing operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Process the necessary values for each batch\n",
    "        us_logits = curModel(x_unsup, training=True)\n",
    "        l_logits = us_logits[:x_sup.shape[0]]\n",
    "\n",
    "        # Compute losses for each respective dataset\n",
    "        sup_loss = sup_loss_fn(y_sup, l_logits)\n",
    "        unsup_loss = weigh_fn(epoch) * tf.reduce_mean(keras.losses.MSE(us_logits, \n",
    "                                yEMA[totalBatch*step:tf.math.minimum(totalBatch*(step+1),totalSamples)]))\n",
    "        total_loss = sup_loss + unsup_loss\n",
    "        \n",
    "        # Update relevant metrics\n",
    "        train_acc_metric.update_state(y_sup, l_logits)\n",
    "        unsup_loss_metric.update_state(unsup_loss)\n",
    "        sup_loss_metric.update_state(sup_loss)\n",
    "        total_loss_metric.update_state(total_loss)\n",
    "        \n",
    "    # Use tape to propagate gradients back to weights\n",
    "    grads = tape.gradient(total_loss, curModel.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, curModel.trainable_weights))\n",
    "    \n",
    "    return us_logits\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    \n",
    "    # iterate accross the batches\n",
    "    for step, (labeled_batch, unlabeled_batch) in enumerate(zip(l_train_ds, ul_train_ds)):\n",
    "        labeled_bx, labeled_by = labeled_batch\n",
    "        unsup_samples = tf.concat([labeled_bx, unlabeled_batch], 0)\n",
    "        \n",
    "        # Progress through a step of training\n",
    "        tAppend = train_step(labeled_bx, unsup_samples, labeled_by, epochVar, stepVar)\n",
    "        if(step == 0):\n",
    "            epLogits = tAppend\n",
    "        else:\n",
    "            epLogits = tf.concat([epLogits, tAppend], 0)\n",
    "        \n",
    "        # Reset metrics and evaluate results\n",
    "        t_acc = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        us_loss = unsup_loss_metric.result()\n",
    "        unsup_loss_metric.reset_states()\n",
    "        s_loss = sup_loss_metric.result()\n",
    "        sup_loss_metric.reset_states()\n",
    "        t_loss = total_loss_metric.result()\n",
    "        total_loss_metric.reset_states()\n",
    "        stepVar = stepVar + 1\n",
    "        \n",
    "    # Update EMA now\n",
    "    if(epoch == 0):\n",
    "        yEMA = epLogits\n",
    "    else:\n",
    "        yEMA = EMA_fn(yEMA, epLogits, epochVar)\n",
    "        \n",
    "    # Print epoch final statistics\n",
    "    if(step == num_batches-1):\n",
    "        print(\"Loss: {:.4f}, US_Loss: {:.4f}, S_Loss: {:.4f}, Acc: {:.2f}\".format(t_loss, us_loss,\n",
    "                                                                                  s_loss, t_acc))\n",
    "    epochVar = epochVar + 1\n",
    "    stepVar = stepVar - stepVar # Assignment causes retracing???? WHY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-shell",
   "metadata": {},
   "source": [
    "The rest function in a similar manner except by including more networks to sample from / train at specific intervals. Keep in mind that while the EMA method did take a bit longer to reach the same accuracy from the previous example, it also requires much more hyperparameters that need adjustment in order to tune the network to get a specific response..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-wallace",
   "metadata": {},
   "source": [
    "And now with the nightmare that was the EMA saved guesses saved... We can move on to the Mean Teacher model, which is admittedly much simpler than what it appears. The teacher model requires two models to be instantiated with the same set of weights, and they slowly begin to diverge as training progresses..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-slovenia",
   "metadata": {},
   "source": [
    "### Mean Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "employed-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function EMA_fn at 0x7fcad6fc4ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "Loss: 1.9789, US_Loss: 0.0457, S_Loss: 1.9332, Acc: 0.36\n",
      "Epoch 2\n",
      "Loss: 1.5585, US_Loss: 0.3317, S_Loss: 1.2268, Acc: 0.60\n",
      "Epoch 3\n",
      "Loss: 1.1895, US_Loss: 0.4101, S_Loss: 0.7793, Acc: 0.76\n",
      "Epoch 4\n",
      "Loss: 0.9023, US_Loss: 0.3643, S_Loss: 0.5380, Acc: 0.82\n",
      "Epoch 5\n",
      "Loss: 0.7144, US_Loss: 0.3016, S_Loss: 0.4129, Acc: 0.87\n",
      "Epoch 6\n",
      "Loss: 0.5800, US_Loss: 0.2569, S_Loss: 0.3231, Acc: 0.92\n",
      "Epoch 7\n",
      "Loss: 0.5180, US_Loss: 0.2316, S_Loss: 0.2865, Acc: 0.91\n",
      "Epoch 8\n",
      "Loss: 0.4137, US_Loss: 0.1972, S_Loss: 0.2165, Acc: 0.95\n",
      "Epoch 9\n",
      "Loss: 0.3810, US_Loss: 0.1753, S_Loss: 0.2056, Acc: 0.95\n",
      "Epoch 10\n",
      "Loss: 0.3388, US_Loss: 0.1587, S_Loss: 0.1801, Acc: 0.95\n",
      "Epoch 11\n",
      "Loss: 0.3026, US_Loss: 0.1481, S_Loss: 0.1545, Acc: 0.96\n",
      "Epoch 12\n",
      "Loss: 0.2797, US_Loss: 0.1410, S_Loss: 0.1388, Acc: 0.96\n",
      "Epoch 13\n",
      "Loss: 0.2613, US_Loss: 0.1292, S_Loss: 0.1321, Acc: 0.96\n",
      "Epoch 14\n",
      "Loss: 0.2309, US_Loss: 0.1207, S_Loss: 0.1102, Acc: 0.96\n",
      "Epoch 15\n",
      "Loss: 0.2052, US_Loss: 0.1146, S_Loss: 0.0906, Acc: 0.98\n",
      "Epoch 16\n",
      "Loss: 0.1874, US_Loss: 0.1085, S_Loss: 0.0789, Acc: 0.98\n",
      "Epoch 17\n",
      "Loss: 0.1855, US_Loss: 0.1017, S_Loss: 0.0837, Acc: 0.97\n",
      "Epoch 18\n",
      "Loss: 0.1827, US_Loss: 0.1003, S_Loss: 0.0824, Acc: 0.98\n",
      "Epoch 19\n",
      "Loss: 0.1688, US_Loss: 0.1006, S_Loss: 0.0682, Acc: 0.98\n",
      "Epoch 20\n",
      "Loss: 0.1786, US_Loss: 0.0935, S_Loss: 0.0851, Acc: 0.97\n",
      "Epoch 21\n",
      "Loss: 0.1438, US_Loss: 0.0911, S_Loss: 0.0527, Acc: 0.99\n",
      "Epoch 22\n",
      "Loss: 0.1502, US_Loss: 0.0878, S_Loss: 0.0624, Acc: 0.98\n",
      "Epoch 23\n",
      "Loss: 0.1510, US_Loss: 0.0975, S_Loss: 0.0535, Acc: 0.98\n",
      "Epoch 24\n",
      "Loss: 0.1460, US_Loss: 0.0823, S_Loss: 0.0636, Acc: 0.98\n",
      "Epoch 25\n",
      "Loss: 0.1315, US_Loss: 0.0792, S_Loss: 0.0523, Acc: 0.99\n",
      "Epoch 26\n",
      "Loss: 0.1262, US_Loss: 0.0835, S_Loss: 0.0426, Acc: 0.99\n",
      "Epoch 27\n",
      "Loss: 0.1336, US_Loss: 0.0855, S_Loss: 0.0481, Acc: 0.99\n",
      "Epoch 28\n",
      "Loss: 0.1135, US_Loss: 0.0735, S_Loss: 0.0400, Acc: 0.99\n",
      "Epoch 29\n",
      "Loss: 0.1129, US_Loss: 0.0745, S_Loss: 0.0384, Acc: 0.99\n",
      "Epoch 30\n",
      "Loss: 0.1384, US_Loss: 0.0805, S_Loss: 0.0579, Acc: 0.98\n",
      "Epoch 31\n",
      "Loss: 0.1157, US_Loss: 0.0740, S_Loss: 0.0417, Acc: 0.99\n",
      "Epoch 32\n",
      "Loss: 0.1091, US_Loss: 0.0735, S_Loss: 0.0356, Acc: 0.99\n",
      "Epoch 33\n",
      "Loss: 0.1129, US_Loss: 0.0712, S_Loss: 0.0417, Acc: 0.99\n",
      "Epoch 34\n",
      "Loss: 0.1240, US_Loss: 0.0717, S_Loss: 0.0523, Acc: 0.99\n",
      "Epoch 35\n",
      "Loss: 0.1181, US_Loss: 0.0698, S_Loss: 0.0483, Acc: 0.99\n",
      "Epoch 36\n",
      "Loss: 0.1102, US_Loss: 0.0699, S_Loss: 0.0403, Acc: 0.99\n",
      "Epoch 37\n",
      "Loss: 0.0857, US_Loss: 0.0607, S_Loss: 0.0250, Acc: 0.99\n",
      "Epoch 38\n",
      "Loss: 0.0918, US_Loss: 0.0672, S_Loss: 0.0246, Acc: 0.99\n",
      "Epoch 39\n",
      "Loss: 0.0955, US_Loss: 0.0666, S_Loss: 0.0289, Acc: 0.99\n",
      "Epoch 40\n",
      "Loss: 0.0910, US_Loss: 0.0648, S_Loss: 0.0261, Acc: 0.99\n",
      "Epoch 41\n",
      "Loss: 0.0873, US_Loss: 0.0630, S_Loss: 0.0243, Acc: 1.00\n",
      "Epoch 42\n",
      "Loss: 0.0833, US_Loss: 0.0597, S_Loss: 0.0235, Acc: 0.99\n",
      "Epoch 43\n",
      "Loss: 0.0810, US_Loss: 0.0577, S_Loss: 0.0233, Acc: 1.00\n",
      "Epoch 44\n",
      "Loss: 0.0788, US_Loss: 0.0569, S_Loss: 0.0219, Acc: 0.99\n",
      "Epoch 45\n",
      "Loss: 0.0845, US_Loss: 0.0615, S_Loss: 0.0230, Acc: 1.00\n",
      "Epoch 46\n",
      "Loss: 0.0984, US_Loss: 0.0650, S_Loss: 0.0335, Acc: 0.99\n",
      "Epoch 47\n",
      "Loss: 0.0833, US_Loss: 0.0618, S_Loss: 0.0216, Acc: 1.00\n",
      "Epoch 48\n",
      "Loss: 0.0751, US_Loss: 0.0610, S_Loss: 0.0141, Acc: 1.00\n",
      "Epoch 49\n",
      "Loss: 0.0776, US_Loss: 0.0564, S_Loss: 0.0213, Acc: 0.99\n",
      "Epoch 50\n",
      "Loss: 0.0827, US_Loss: 0.0559, S_Loss: 0.0268, Acc: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Implement Mean Teacher Training\n",
    "teacherMod = LeNet()\n",
    "studentMod = LeNet()\n",
    "sup_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-3)\n",
    "epochVar = tf.Variable(0)\n",
    "\n",
    "# Metrics\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "unsup_loss_metric = keras.metrics.Mean()\n",
    "sup_loss_metric = keras.metrics.Mean()\n",
    "total_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "# Implement the values chosen for the weighting function\n",
    "w_values = tf.constant(np.linspace(0, DISTANCE_WEIGHT_MAX, DISTANCE_MAX_WEIGHT_EPOCH), dtype='float32')\n",
    "\n",
    "# Initialize teacher with student weights\n",
    "teacherMod.set_weights(studentMod.get_weights())\n",
    "\n",
    "# Set up EMA func for weights\n",
    "# NOTE: THIS FUNCTION WILL BE RETRACED AT LEAST AS MANY TIMES AS THERE ARE LAYERS WITH DIFFERENTLY\n",
    "#       SHAPED WEIGHT MATRICES!!!\n",
    "EMA_RATE = tf.constant(0.5)\n",
    "@tf.function\n",
    "def EMA_fn(toUpdate, update):\n",
    "    return EMA_RATE*toUpdate + (1-EMA_RATE)*update\n",
    "\n",
    "# Now set up weighing function\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def weigh_fn(epoch):\n",
    "    return w_values[tf.math.minimum(DISTANCE_MAX_WEIGHT_EPOCH-1, epoch)]\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_sup, x_unsup, y_sup, epoch):\n",
    "    # Calculate gradients while performing operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Process the necessary values for each batch\n",
    "        us_slogits = studentMod(x_unsup, training=True)\n",
    "        us_tlogits = teacherMod(x_unsup, training=True)\n",
    "        l_logits = us_slogits[:x_sup.shape[0]]\n",
    "\n",
    "        # Compute losses for each respective dataset\n",
    "        sup_loss = sup_loss_fn(y_sup, l_logits)\n",
    "        unsup_loss = weigh_fn(epoch) * tf.reduce_mean(keras.losses.MSE(us_slogits, us_tlogits))\n",
    "        total_loss = sup_loss + unsup_loss\n",
    "        \n",
    "        # Update relevant metrics\n",
    "        train_acc_metric.update_state(y_sup, l_logits)\n",
    "        unsup_loss_metric.update_state(unsup_loss)\n",
    "        sup_loss_metric.update_state(sup_loss)\n",
    "        total_loss_metric.update_state(total_loss)\n",
    "        \n",
    "    # Use tape to propagate gradients back to weights\n",
    "    grads = tape.gradient(total_loss, studentMod.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, studentMod.trainable_weights))\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    \n",
    "    # iterate accross the batches\n",
    "    for step, (labeled_batch, unlabeled_batch) in enumerate(zip(l_train_ds, ul_train_ds)):\n",
    "        labeled_bx, labeled_by = labeled_batch\n",
    "        unsup_samples = tf.concat([labeled_bx, unlabeled_batch], 0)\n",
    "        \n",
    "        # Progress through a step of training\n",
    "        train_step(labeled_bx, unsup_samples, labeled_by, epochVar)\n",
    "        \n",
    "        # Apply the EMA for the teacher\n",
    "        teacherMod.set_weights([EMA_fn(tWeight, sWeight) for (tWeight, sWeight) in zip(teacherMod.get_weights(),\n",
    "                                                                                       studentMod.get_weights())])\n",
    "        \n",
    "        # Reset metrics and evaluate results\n",
    "        t_acc = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        us_loss = unsup_loss_metric.result()\n",
    "        unsup_loss_metric.reset_states()\n",
    "        s_loss = sup_loss_metric.result()\n",
    "        sup_loss_metric.reset_states()\n",
    "        t_loss = total_loss_metric.result()\n",
    "        total_loss_metric.reset_states()\n",
    "        \n",
    "        # Print epoch final statistics\n",
    "        if(step == num_batches-1):\n",
    "            print(\"Loss: {:.4f}, US_Loss: {:.4f}, S_Loss: {:.4f}, Acc: {:.2f}\".format(t_loss, us_loss,\n",
    "                                                                                      s_loss, t_acc))\n",
    "        epochVar = epochVar + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-drink",
   "metadata": {},
   "source": [
    "### Dual Students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-helmet",
   "metadata": {},
   "source": [
    "At this point, it should be easy to follow up on how to implement the dual students model. It will be nearly exactly the same as above but with an additional student model. Which student model is updated will depend largely on the distance of each model from the teacher (it is possible to update both or just one of them), and the teacher will be updated in the same way it was before.\n",
    "\n",
    "Note that there are obviously some tradeoffs with all of these models in terms of space. While the first is the most unstable, it's the most free to use in terms of space requirements. On the other hand, temporal ensembling will require __as much space as the expected output__ in order to train (which can be huge with things like segmentation maps!), and mean teacher will require as __double the amount of space to store weights__ to store two networks on the same GPU during training (which will obviously limit the total size of the network that will be used for training...). Keep that in mind when approaching these problems!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
