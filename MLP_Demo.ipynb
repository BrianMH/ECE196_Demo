{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for this walkthrough\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as the baseline callback for the training session\n",
    "# A bit buggy due to the way that accuracy is measured\n",
    "class TerminateOnBaseline(keras.callbacks.Callback):\n",
    "    \"\"\"Callback that terminates training when either acc or val_acc reaches a specified baseline\"\"\"\n",
    "    def __init__(self, monitor, baseline):\n",
    "        super(TerminateOnBaseline, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(self.monitor)\n",
    "        if acc is not None:\n",
    "            if acc >= self.baseline:\n",
    "                print('Epoch {}: Reached baseline with value {}, terminating training on next iteration'.format(epoch+1, logs[self.monitor]))\n",
    "                self.model.stop_training = True\n",
    "\n",
    "# Plots a 2D decision boundary given the input training data\n",
    "def plotDecisionBoundary(predFx, XTrain, YTrain, relDistVal = 1, stepSize = 0.025):\n",
    "    x_min, x_max = XTrain[:, 0].min() - relDistVal, XTrain[:, 0].max() + relDistVal\n",
    "    y_min, y_max = XTrain[:, 1].min() - relDistVal, XTrain[:, 1].max() + relDistVal\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, stepSize),\n",
    "                         np.arange(y_min, y_max, stepSize))\n",
    "\n",
    "\n",
    "    # Plot decision boundary\n",
    "    fig, ax = plt.subplots()\n",
    "    Z = predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    ax.scatter(XTrain[:, 0], XTrain[:, 1], c=YTrain, cmap=plt.cm.RdBu)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Decision Boundary for XOR Problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny TensorFlow Tutorial\n",
    "Most of this information can be found on the Face Recognition tutorial which you can find [here](https://sites.google.com/view/ece-196/face-recognition/intro-to-keras). It will not be necessary to read this guide for this particular section, but it will become largely useful for the CNN portion of this class.\n",
    "\n",
    "## Functional Paradigm\n",
    "Currently, the most convenient way to implement TensorFlow models is to use the eager execution environment. While there are some benefits to executing static graphs for a self-contained model, eager execution allows stop-point execution of TF code. This generally means that lines that correspond to specific operations do indeed get executed in a predictable way. In our case, we simply use the eager environment because there's no downside. Furthermore, since Keras has become a first-class citizen in TensorFlow, we will also be using it to simplify the creation of models that use basic layers.\n",
    "\n",
    "However, before we get into the design of models using these predefined layers, we should establish how exactly programming using TensorFlow works. To summarize the page above, the typical paradigm for creating networks using TensorFlow is the functional paradigm. The reason it is known as the functional paradigm is due to the way that the data flow is determined by the use of layer functors (see the Python introduction in the FR guide). Each operation can be summarized as a layer instantiation followed by a layer execution in the following manner:\n",
    "\n",
    "```python\n",
    "output = LayerType(initPosArgs*, initKWArgs**)(input)\n",
    "```\n",
    "\n",
    "In the above example, the initArgs represents the possible positional and keword arguments that can be passed in order to initialize the LayerType layer. Finally, the input to the layer is passed into the layer immediately following the intialization of the layer through a function call. This will be the way most functions function when using the Keras interface of TensorFlow, and the TF environment has made this type of operation very consistent for even activation layers. At this point, I strongly suggest looking at the above introduction page in order to familiarize yourself with the functions that will be used in this session. Note that the only layer that will be used for this particular notebook is the Dense layer, which functions as a fully connected layer.\n",
    "\n",
    "## Relevant Layers\n",
    "Please take a look at [this](https://sites.google.com/view/ece-196/face-recognition/intro-to-keras#h.p_sV-YQ6VxoL_r) subsection of the Keras tutorial in order to see what layers will be relevant for this assignment. The only other layer which is not covered in the linked guide is the input layer, which will be covered in the example below. For now, it's more important to understand the connections between the layer implementation and the conceptual idea of the layer than to memorize the entire API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the XOR Problem \n",
    "## Introduction\n",
    "Before starting, recall the typical formulation for a single layer neural network as seen below, where $X$ is our input feature array, $\\phi$ is the activation function, and $W$ defines the set of weights used in the network:\n",
    "![Neural Model](./images/singlelayer.png)\n",
    "\n",
    "\n",
    "$$ Y = \\phi\\left(WX+b\\right) $$\n",
    "\n",
    "These \"layers\" of neurons form the building blocks of basic feed-forward neural networks. Thus, an implementation for a three layer neural network would look as follows (where $P$ is simply whatever set of preprocessed features we have):\n",
    "![Neural Model 3](./images/threelayer.png)\n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "X &\\leftarrow P \\\\\n",
    "A_1 &= \\phi_1\\left(W_1X+b_1\\right) \\\\\n",
    "A_2 &= \\phi_2\\left(W_2A_1+b_2\\right) \\\\\n",
    "Y &= \\phi_3\\left(W_3A_2+b_3)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Since we are solving a classification problem, we decide to let the final activation function be a sigmoid (more formally, $\\phi_3(x)=\\frac{1}{1+e^{-x}}$). The inner layers we let be a simple ReLu activation ($\\phi_1(x)=\\phi_2(x)=\\max\\left(0,x\\right)$). Since a sigmoid can be treated as the probability of an input pertaining to some class, $P(Y=1|X=x)$, this should suffice for the network. If you originally thought of using a multi-class output instead (and thus used a softmax on two outputs neurons instead), notice that by the definition of the softmax function, both of these will simply be complements and thus contain redundant information about the class the sample corresponds to. That being said, the training process would function very similarly due to this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Specification\n",
    "The XOR is a classification problem that can be represented with the following visual:\n",
    "![Problem](./images/xorProb.png)\n",
    "\n",
    "In other words, we have a set of four points as the possible inputs and wish to train a network in order to be able to identify the class that each of these points corresponds to correctly and deterministically (that is, we don't want the answer to change if we query the network about the same point!). As shown in the lecture slides, there is no linear hyperplane that can be used as the decision boundary for this particular classification problem. Naturally, then, one might consider using a neural network due to its abilities to incorporate non-linearity into its decision boundary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Prepares NP arrays for the input data\n",
    "inDataset = np.array([[1, 0], [0, 1], [0, 0], [1, 1]])\n",
    "labels = np.array([1, 1, 0, 0]).astype(dtype=np.int)\n",
    "\n",
    "# Size of the inputs/labels\n",
    "print(inDataset.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "With the math laid out above, we can now start implementing a concrete network. Since Keras has since become the dominant way of implementing models using pre-defined layers, we will be using the simpler Keras interface in conjunction with the normal TensorFlow session for the rest of this notebook. So, given the algorithmic view of the three-layer network above, we fill in the blanks and decide upon the following set of properties for use in the network:\n",
    "\n",
    "  1. Placeholder [Input: $2$] (Reason: $x\\in\\mathcal{R}^2$)\n",
    "  2. Fully Connected Layer [Output: $8$, Activation: ReLu]\n",
    "  3. Fully Connected Layer [Output: $8$, Activation: ReLu]\n",
    "  4. Output $\\rightarrow$ Fully Connected Layer [Output: $2$, Activation: Softmax] (Reason: $y_i=P(Y=i|X=x),i\\in\\{0,1\\}$)\n",
    "  \n",
    "In other words, this network will be trained in order to take in some two-dimensional input and output a set of normalized probabilities that correspond to the chances of the output pertaining to some class given the input. The following covers the model generation using Keras layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Implements the network using TensorFlow.Keras\n",
    "def MLPNetwork():\n",
    "    # input layer (note that batches are already taken care of for you)\n",
    "    x = keras.Input(shape=(2,))\n",
    "    \n",
    "    # subsequent dense layers\n",
    "    a1 = keras.layers.Dense(8, activation='relu')(x)\n",
    "    a2 = keras.layers.Dense(8, activation='relu')(a1)\n",
    "    \n",
    "    # classification dense layer\n",
    "    logits = keras.layers.Dense(1, activation='sigmoid')(a2)\n",
    "    \n",
    "    # Creates the model given the above structure\n",
    "    model = keras.Model(inputs=x, outputs=logits)\n",
    "    return model\n",
    "\n",
    "MLPmod = MLPNetwork()\n",
    "MLPmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "With the model declared and the proper summary output, we can now use our input data and labels to train the network.\n",
    "\n",
    "Generally, with Tensorflow, optimization follows a series of steps as follows:\n",
    "\n",
    "1. Declare your model [Finished]\n",
    "2. Declare the optimizer\n",
    "3. Declare the loss function\n",
    "4. Compile the model using the above (or manage the gradient tapes manually)\n",
    "5. Fit the compiled model on the input training data\n",
    "\n",
    "Given our simple problem, there is no reason to choose any particular optimizer. To make things simpler, we choose the vanilla SGD optimizer as it requires the least fussing around. The loss function is clearly going to be the categorial cross-entropy loss as our problem is a multi-class classification problem. Finally, since we do not have any business dealing with the gradients manually, we can use the compile function to deal with the gradient application on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 110ms/sample - loss: 0.7181 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6995 - accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6904 - accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6825 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6792 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6748 - accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6693 - accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6653 - accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6629 - accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6597 - accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6548 - accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6498 - accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6456 - accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6468 - accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.6372 - accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.6337 - accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6272 - accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6219 - accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6169 - accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6109 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.6038 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.5902 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.5837 - accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.5728 - accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.5770 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.5567 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.5496 - accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.5474 - accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 997us/sample - loss: 0.5376 - accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.5220 - accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "Epoch 31: Reached baseline with value 1.0, terminating training on next iteration\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.5057 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Sets up the optimizer for use\n",
    "opt = keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "# Trains the network using the given input data and labels\n",
    "MLPmod.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# The callback forces the network to train until it reaches 100% accuracy\n",
    "hist = MLPmod.fit(inDataset, labels, epochs=100, callbacks=[TerminateOnBaseline(monitor=\"accuracy\", baseline=1.0),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Analysis\n",
    "As seen above, despite the seemingly large learning rate, the network was able to succesfully train itself to a 100% accuracy. Notice, however, that the loss is still non-zero. This makes sense since the surface that we are training the network on (probabilities), has a large range of values, and all that is necessary in order to ensure the proper answer is that $P(Y=i_{\\text{true}}|X=x)\\ge0.5$ for the true class of sample $x$, $i_{\\text{true}}$. Further training can possible lead to a smaller loss value, but further training may also lead to the possibility of overfitting the training dataset.\n",
    "\n",
    "With this in mind, we would like to see the decision boundary out of interest. Even though our network has succesfully trained on the training set, looking at the decision boundary will tell us about the amount of effort that the network put into learning this function mapping and whether or not it makes sense for the decision boundary to look as such...\n",
    "\n",
    "__NOTE:__ The network above may sometimes give an accuracy of 1.0 but still be incorrect once plotted by the decision boundary function. Since the accuracy function rounds logits values, it's possible for the network to confuse a value close to $0.5$ as potentially coming from either class and thus contributing the the overall accuracy despite being incorrect. If this happens, just rerun the notebook so it learns a new function or run the box above one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Designate a prediction lambda to form our predictions from an input\n",
    "predict = lambda x_test:np.round(MLPmod.predict(x_test))\n",
    "\n",
    "# Now plot this decision boundary across some uniform set of X and Y coordinates to visualize the decision boundary    \n",
    "plotDecisionBoundary(predict, inDataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "One of the most important questions to ask after training a network is the following: Does the output make any sense? If you run the above code many times, you will see that many different types of decision boundaries are generated for this problem. More likely than not, all of these boundaries will have been able to solve the XOR problem. So, yes, the output makes sense in the context of the XOR problem.\n",
    "\n",
    "The real problem with answering this question comes with generalization, however. The XOR problem has properly generalized to the input domain space (specifically the four points defining the XOR problem), but, say that instead of being limited to this space, we have a continuum of points on the line created by the two red dots. That is, there are infinitely many points corresponding to the blue and red classes. It should be clear that if the true form of one class was a line on the 2D plane, then the neural network can possible fail to generalize properly, despite the points given being a proper representation of that problem as well. An example of this failure can be seen in the decision boundary below:\n",
    "![Possible Result](toothpick.png)\n",
    "\n",
    "Make sure to keep this in mind. Your network results are heavily dependent on the way that the input dataset is set up. If you give a dataset that is not well representative of your input space (in the XOR case, we actually used ALL possible points of the input domain!), then you will likely produce garbage results that form either due to an overfitting of points or a fitting of an incorrect data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
